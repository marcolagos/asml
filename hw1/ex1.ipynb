{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1913e4d9",
   "metadata": {},
   "source": [
    "# Homework 1: Problem 4: Linear regression in PyTorch\n",
    "## Predicting home prices with the California housing dataset   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561d2e4d",
   "metadata": {},
   "source": [
    "This file contains code that helps you get started on linear regression as a neural network with one input layer with N units corresponding to the number of predictors, and an output layer with one unit. You will implement linear regression to predict the median value of a home in a census block in California from  eight predictor variables. This notebook has already been set up to load this data for you and convert it into a Pandas dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6e732b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (8.0,5.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'viridis'\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07fed54",
   "metadata": {},
   "source": [
    "## Reading data and plotting\n",
    "\n",
    "We download the California housing dataset, and load it into a pandas dataframe. We have data on median home price in census block groups in California. We want to predict the median home price on the basis of eight numeric predictors at the block group level. \n",
    "\n",
    "- MedInc (median income in block group)\n",
    "\n",
    "- HouseAge (median house age in block group)\n",
    "\n",
    "- AveRooms (average number of rooms per household)\n",
    "\n",
    "- AveBedrms (average number of bedrooms per household)\n",
    "\n",
    "- Population (block group population)\n",
    "\n",
    "- AveOccup (average number of household members)\n",
    "\n",
    "- Latitude (block group latitude)\n",
    "\n",
    "- Longitude (block group longitude)\n",
    "\n",
    "Before starting on any task, it is often useful to understand the data by visualizing it. For this\n",
    "dataset, we will show you how to use histograms and scatter plots to visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18d8e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the California Housing data and place it in a pandas's dataframe\n",
    "\n",
    "cdata = fetch_california_housing()\n",
    "predictors = pd.DataFrame(data = cdata.data, columns = cdata.feature_names)\n",
    "y = pd.DataFrame(cdata.target,columns = ['price'])\n",
    "cal_df = pd.concat([predictors,y],axis=1)\n",
    "cal_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a0a754",
   "metadata": {},
   "source": [
    "# Data visualization\n",
    "- plot the distribution of the target variable \n",
    "    - note that all home prices over 5 are assigned a value of 5.\n",
    "    - distribution is not Gaussian, right skew with outliers at the high end.\n",
    "    \n",
    "- plot the predictor variables\n",
    "    - AveRooms, AveBedrooms, Population, AveOccupancy have extreme values; the range of values is high, quite a few outliers. Unlikely to be useful predictors, because almost all homes cluster at the very low end of values!\n",
    "    - MedInc, HouseAge, Latitude, Longitude seem more promising as predictors, with nice distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf485143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution of the target variable\n",
    "sns.histplot(cal_df['price']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363e30bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the distributions of the individual predictor variables\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "cal_df.hist(figsize=(12, 10), bins=30, edgecolor=\"black\")\n",
    "plt.subplots_adjust(hspace=0.7, wspace=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e26618",
   "metadata": {},
   "source": [
    "# Spatial visualization\n",
    "- prices have been discretized by the scatterplot function to simplify the legend.\n",
    "- most expensive homes are along the coast and in Sacramento, as expected. \n",
    "- Latitude and Longitude are very likely to be good predictors of home value. (location, location, location!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d11a127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing the spatial distribution of homes\n",
    "sns.scatterplot(\n",
    "    data=cal_df,\n",
    "    x=\"Longitude\",\n",
    "    y=\"Latitude\",\n",
    "    size=\"price\",\n",
    "    hue=\"price\",\n",
    "    palette=\"viridis\",\n",
    "    alpha=0.5,\n",
    ")\n",
    "plt.legend(title=\"Price\", bbox_to_anchor=(1.05, 0.95), loc=\"upper left\")\n",
    "plt.title(\"Median home price as a function of spatial location\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683ec73d",
   "metadata": {},
   "source": [
    "# PyTorch model building (10 points)\n",
    "- Create the model (Read [3.5.1](https://d2l.ai/chapter_linear-regression/linear-regression-concise.html#defining-the-model))\n",
    "    - define the network structure: 1 layer network (4 points)\n",
    "       - set the learning rate lr, and number of inputs num_inputs\n",
    "       - use [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear) with num_inputs inputs and one output to set up self.net\n",
    "       - initialize the weights of network to numbers drawn from a zero-mean normal distribution with std = noise (use self.net.weight.data.normal_(mean,std))\n",
    "       - initialize bias weight to 0 (use self.net.bias.data.fill_(val))\n",
    "    - define the forward function (Read 3.5.1) (2 points)\n",
    "       - propagate the input X through the net\n",
    "    - define the loss function (mean squared error) (Read 3.5.2) (2 points)\n",
    "       - [nn.functional.mse_loss](https://pytorch.org/docs/stable/generated/torch.nn.functional.mse_loss.html)\n",
    "    - configure the optimizer (Read 3.5.3) (2 points)\n",
    "       - use [torch.optim.SGD()](https://pytorch.org/docs/stable/optim.html) with self.parameters() and set learning to be the learning rate of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2f7ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, num_inputs, lr, noise=0.01):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.num_inputs, self.lr = num_inputs, lr\n",
    "        self.net = nn.Linear(num_inputs, 1)\n",
    "        self.weights = self.net.weight.data.normal_(0, noise)\n",
    "        self.bias = self.net.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.net(X)\n",
    "    \n",
    "    def loss(self, yhat, y):\n",
    "        return nn.functional.mse_loss(yhat, y)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.SGD(self.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01901270",
   "metadata": {},
   "source": [
    "## Test your model set up code\n",
    "- run this code after completing the functions above.\n",
    "- you should be able to view your model's structure and its weights and bias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14a59dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RegressionModel(cal_df.shape[1]-1,1)\n",
    "print(model)\n",
    "# access model's weights and bias\n",
    "print(model.net.weight)\n",
    "print(model.net.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a3820a",
   "metadata": {},
   "source": [
    "# Splitting and standardizing training data (6 points)\n",
    "- split training data (X,y) into a training set (XXtrain,yytrain) and (Xtest,ytest) using sklearn's [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) (with test_size = 0.2)\n",
    "- If standardize is True, use sklearn.preprocessing StandardScaler() and fit to XXtrain. Then apply the scaler to transform both XXtrain and Xtest. \n",
    "- Split (XXtrain,yytrain) further into (Xtrain,ytrain) and (Xval,yval) using using sklearn's train_test_split (with test_size = 0.2)\n",
    "- Convert Xtrain,Xval,Xtest,ytrain,yval,ytest into Torch tensors, using [torch.Tensor()](https://pytorch.org/docs/stable/tensors.html)\n",
    "- Return tensors Xtrain,Xval,Xtest,ytrain,yval,ytest\n",
    "   - the shapes of the y tensors should be of the form torch.Size([N,1]) [**reshape** if needed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d00488c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_val_test_split(X, y, standardize=True):\n",
    "    # split X,y into a training and test set in an 80/20 ratio.\n",
    "    XXtrain, Xtest, yytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=7)\n",
    "    \n",
    "    # standardize training and test set if flag is True\n",
    "    if standardize:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(XXtrain)\n",
    "        XXtrain = scaler.transform(XXtrain)\n",
    "        Xtest = scaler.transform(Xtest)\n",
    "\n",
    "    # split the training set further into train and val sets in an 80/20 ratio\n",
    "    Xtrain, xval, ytrain, yval = train_test_split(XXtrain, yytrain, test_size=0.2, random_state=7)\n",
    "\n",
    "    # convert train data to tensors\n",
    "    Xtrain_tensor = torch.tensor(Xtrain, dtype=torch.float32)\n",
    "    ytrain_tensor = torch.tensor(ytrain, dtype=torch.float32)\n",
    "\n",
    "    # convert val data to tensors\n",
    "    Xval_tensor = torch.tensor(xval, dtype=torch.float32)\n",
    "    yval_tensor = torch.tensor(yval, dtype=torch.float32)\n",
    "\n",
    "    # convert test data to tensors\n",
    "    Xtest_tensor = torch.tensor(Xtest, dtype=torch.float32)\n",
    "    ytest_tensor = torch.tensor(ytest, dtype=torch.float32)\n",
    "\n",
    "    return Xtrain_tensor, Xval_tensor, Xtest_tensor, ytrain_tensor, yval_tensor, ytest_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5348a90",
   "metadata": {},
   "source": [
    "## Test the train/val/test split function\n",
    "- run the cell below after implementing the function in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05627b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cal_df.iloc[:,:-1].values # predictor matrix\n",
    "y = cal_df['price'].values    # target vector\n",
    "\n",
    "Xtrain_tensor,Xval_tensor,Xtest_tensor,ytrain_tensor,yval_tensor,ytest_tensor = \\\n",
    "        make_train_val_test_split(X,y,standardize=True)\n",
    "print('Train data: ', Xtrain_tensor.shape, ytrain_tensor.shape)\n",
    "print('Val data: ', Xval_tensor.shape, yval_tensor.shape)\n",
    "print('Test data: ', Xtest_tensor.shape, ytest_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2253a42f",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "The train function takes the predictor matrix X and the target vector y, the learning rate lr, and the number of epochs, num_epochs,  to train as inputs. Instead of using D2L's trainer class (explained in Section 3.2), we actually write out the training loop explicitly to be able to understand the steps in the training process. Here is the algorithm.\n",
    "\n",
    "1. Construct tensors Xtrain_tensor,Xval_tensor,Xtest_tensor,ytrain_tensor,yval_tensor,ytest_tensor by calling make_train_val_test_split() with predictor vector X and target vector y (and standardize=True).\n",
    "2. Set up the model by instantiating RegressionModel with the number of predictors in Xtrain_tensor, and learning rate lr\n",
    "3. Configure the optimizer for the model by calling the configure_optimizers() method defined in the RegressionModel class.\n",
    "4. Initialize torch tensors of length num_epochs to store train set loss and val set loss (for plotting)\n",
    "5. For epoch in range(num_epochs)\n",
    "   - zero the parameter gradients in the optimizer (use zero_grad())\n",
    "   - perform a forward pass, and calculate loss on train set (model.loss()). Store train set loss in train set loss tensor.\n",
    "   - do a backward step to calculate gradients (loss.backward())\n",
    "   - get the optimizer to take a step in direction of the gradient (optimizer.step())\n",
    "   - calculate loss on val set and store in val set loss tensor\n",
    "6. Plot train set loss and val set loss on y-axis and epoch number on x-axis. These are the training curves.\n",
    "7. return the regression model object, and the Xtest tensor and ytest tensor. The last two are for model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ed83ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_model(X,y,model,num_epochs):\n",
    "    \n",
    "    # split into train, val, test sets\n",
    "    Xtrain_tensor, Xval_tensor, Xtest_tensor, ytrain_tensor, yval_tensor, ytest_tensor = make_train_val_test_split(X, y, standardize=True)\n",
    "\n",
    "    model = RegressionModel(Xtrain_tensor.shape[1], lr=0.1)\n",
    "    \n",
    "    # configure optimizer for model\n",
    "    optimizer = model.configure_optimizers()\n",
    "\n",
    "    # initialize train loss and val loss tensors for for plotting\n",
    "    train_set_loss = torch.zeros(num_epochs)\n",
    "    val_set_loss = torch.zeros(num_epochs)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "       # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize step\n",
    "        fpass = model.forward(Xtrain_tensor)\n",
    "        loss = model.loss(fpass, ytrain_tensor)\n",
    "        train_set_loss[epoch] = loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # calculate and store train loss and val loss for the epoch\n",
    "        fpass = model.forward(Xval_tensor)\n",
    "        loss = model.loss(fpass, yval_tensor)\n",
    "        val_set_loss[epoch] = loss\n",
    "        \n",
    "\n",
    "    # plot train set loss and val set loss as a function of epoch\n",
    "    plt.plot(range(num_epochs), train_set_loss.tolist(), label=\"Train Loss\")\n",
    "    plt.plot(range(num_epochs), val_set_loss.tolist(), label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    " \n",
    "    # print values of train set loss and val set loss on the last epoch\n",
    "    print(f\"Final Train Loss: {train_set_loss[-1]}\")\n",
    "    print(f\"Final Validation Loss: {val_set_loss[-1]}\")\n",
    "    \n",
    "    return model,Xtest_tensor,ytest_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5b6ba7",
   "metadata": {},
   "source": [
    "# Analyze model performance (10 points)\n",
    "- get histogram of errors (actual price - predicted price) on set aside test data (Xtest_tensor,ytest_tensor) (3 points)\n",
    "- get scatterplot of actual price vs predicted price on set aside test data (3 points)\n",
    "- compute R2 of concordance between actual and predicted values using sklearn.metrics [r2_score()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html) function. (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b348bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# error distribution on test data (2 lines)\n",
    "model, Xtest_tensor, ytest_tensor = create_train_test_model(X, y, model, 100)\n",
    "with torch.no_grad():\n",
    "    output = model.forward(Xtest_tensor)\n",
    "    error = output - ytest_tensor.view(-1, 1)\n",
    "\n",
    "error = [value[0] for value in error]\n",
    "\n",
    "# plot hisogram (3 lines)\n",
    "plt.hist(error, bins=30, edgecolor=\"black\")\n",
    "plt.xlabel(\"Error\")\n",
    "plt.show()\n",
    "\n",
    "# scatter plot of predicted vs actual price (4 lines)\n",
    "output_np = output.numpy()  # Convert output tensor to a NumPy array\n",
    "ytest_np = ytest_tensor.view(-1, 1).numpy()  # Convert ytest_tensor to a NumPy array\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=output_np.flatten(), y=ytest_np.flatten())  # Flatten the arrays\n",
    "plt.xlabel(\"Predicted Price\")\n",
    "plt.ylabel(\"Actual Price\")\n",
    "plt.title(\"Predicted vs Actual Price\")\n",
    "plt.show()\n",
    "    \n",
    "# assess model quality using r2 (1 line)\n",
    "print(f\"R2 Score: {metrics.r2_score(ytest_np, output_np)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e281c1c1",
   "metadata": {},
   "source": [
    "# Visualize model parameters\n",
    "- run code in the following two cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34550efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "print('Model weights:', model.net.weight)\n",
    "print('Model bias:', model.net.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49391f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot model coefficients\n",
    "coefs = pd.DataFrame(model.net.weight.data.detach().numpy(),columns=cal_df.columns[:-1])\n",
    "sns.barplot(y=coefs.columns, x=coefs.iloc[0].values,orient='h')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fda79d3",
   "metadata": {},
   "source": [
    "# Cross-validation to assess model quality (10 points + 10 points)\n",
    "\n",
    "Up to now, we have assessed model quality with a single train/test split. Now we use a more systematic approach based on 5-fold crossvalidation.  The $R^2$ value is used as a measure of model quality. The closer $R^2$ is to 1., the better the model is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32dc3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful for cross-validation, here we split the train folds into a train set and a validation set and \n",
    "# a test set \n",
    "# train_index and test_index are indices of examples in (X,y) that are in training set and test set \n",
    "# (generated by the split function on sklearn's KFold)\n",
    "\n",
    "def make_train_val_test_cv_split(X,y,train_index,test_index,standardize=True):\n",
    "    \n",
    "    # construct the training and test subsets of X,y (2 lines) using the indices\n",
    "    # XXtrain, Xtest = X[0:train_index], X[train_index: test_index]\n",
    "    # yytrain, ytest = y[0:train_index], y[train_index: test_index]\n",
    "    \n",
    "    # if standardize flag is set, fit a Standard Scaler on training data and transform the \n",
    "    # training and test X (3 lines)\n",
    "    if standardize:\n",
    "        scaler = StandardScaler()\n",
    "        XXtrain = scaler.fit_transform(XXtrain)\n",
    "        Xtest = scaler.transform(Xtest)\n",
    "\n",
    "    # split the training X,y set into train and val sets (1 line)\n",
    "    Xtrain, xval, ytrain, yval = train_test_split(XXtrain, yytrain, test_size=0.2, random_state=7)\n",
    "    \n",
    "    # convert train data to tensors (2 lines)\n",
    "    Xtrain_tensor = torch.tensor(Xtrain, dtype=torch.float32)\n",
    "    ytrain_tensor = torch.tensor(ytrain, dtype=torch.float32)\n",
    "\n",
    "    # convert val data to tensors (2 lines)\n",
    "    Xval_tensor = torch.tensor(xval, dtype=torch.float32)\n",
    "    yval_tensor = torch.tensor(yval, dtype=torch.float32)\n",
    "\n",
    "    # convert test data to tensors (2 lines)\n",
    "    Xtest_tensor = torch.tensor(Xtest, dtype=torch.float32)\n",
    "    ytest_tensor = torch.tensor(ytest, dtype=torch.float32)\n",
    "    \n",
    "    return Xtrain_tensor,Xval_tensor,Xtest_tensor,ytrain_tensor,yval_tensor,ytest_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db0aa02",
   "metadata": {},
   "source": [
    "# Training in crossvalidation mode \n",
    "\n",
    "X,y are predictor matrix and target vector. K is the number of crossvalidation folds. Models is a list of length K of regression model objects. num_epochs is the number of epochs to train each model\n",
    "\n",
    "1. Create a KFold() object called kfolds with n_splits = K\n",
    "2. For (train_index,test_index) in kfolds.split(X)\n",
    "\n",
    "   - Construct tensors Xtrain_tensor,Xval_tensor,Xtest_tensor,ytrain_tensor,yval_tensor,ytest_tensor by calling make_train_val_test_split() with predictor vector X and target vector y (and standardize=True).\n",
    "   - Extract model from models for that fold\n",
    "   - Configure the optimizer for the model for that fold by calling the configure_optimizers() method defined in the RegressionModel class.\n",
    "   - Initialize torch tensors of length num_epochs to store train set loss and val set loss (for plotting)\n",
    "   - For epoch in range(num_epochs)\n",
    "       - zero the parameter gradients in the optimizer (use zero_grad())\n",
    "       - perform a forward pass, and calculate loss on train set (model.loss()). Store train set loss in train set loss tensor.\n",
    "       - do a backward step to calculate gradients (loss.backward())\n",
    "       - get the optimizer to take a step in direction of the gradient (optimizer.step())\n",
    "       - calculate loss on val set and store in val set loss tensor\n",
    "   - Plot train set loss and val set loss on y-axis and epoch number on x-axis. These are the training curves for that fold.\n",
    "3. return the regression models list, and the Xtest tensor and ytest tensor. The last two are for model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d0ab64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_cv_model(X,y,K,models,num_epochs):\n",
    "    \n",
    "    # make the kfolds object by calling sklearn's KFold with parameter K\n",
    "    kfolds = KFold(n_splits=K)\n",
    "    \n",
    "    # for every fold\n",
    "    for (train_index,test_index) in kfolds.split(X):\n",
    "    \n",
    "        # split into train, val, test sets (1 line)\n",
    "        Xtrain_tensor,Xval_tensor,Xtest_tensor,ytrain_tensor,yval_tensor,ytest_tensor = \\\n",
    "        make_train_val_test_cv_split(X,y,train_index,test_index,standardize=True)\n",
    "        \n",
    "      \n",
    "        # extract model for that fold from list of models (2 lines)\n",
    "        # set up optimizer\n",
    "        kfolds_model = models[0]\n",
    "        optimizer = kfolds_model.configure_optimizers()\n",
    "        \n",
    "        # initialize train_loss and val_loss tensors (2 lines)\n",
    "        train_set_loss = torch.zeros(num_epochs)\n",
    "        val_set_loss = torch.zeros(num_epochs)\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            # zero the parameter gradients (1 line)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize (4 lines)\n",
    "            fpass = kfolds_model.forward(Xtrain_tensor)\n",
    "            loss = kfolds_model.loss(fpass, ytrain_tensor)\n",
    "            train_set_loss[epoch] = loss\n",
    "            \n",
    "            # calculate and store train and val loss for that epoch (2 lines)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        # plot train loss and val loss curves for that fold (2 lines)\n",
    "        plt.plot(range(num_epochs), train_set_loss.tolist(), label=\"Train Loss\")\n",
    "        plt.plot(range(num_epochs), val_set_loss.tolist(), label=\"Validation Loss\")\n",
    "        \n",
    "        # print final train and val loss for that fold (2 lines)\n",
    "        print(f\"Final Train Loss: {train_set_loss[-1]}\")\n",
    "        print(f\"Final Validation Loss: {val_set_loss[-1]}\")\n",
    "        \n",
    "    return models,Xtest_tensor,ytest_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c0bcf1",
   "metadata": {},
   "source": [
    "## Test your crossvalidation trainer\n",
    "- run the cell below after completing the two functions above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7e8e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cal_df.iloc[:,:-1].values # predictor matrix\n",
    "y = cal_df['price'].values    # target vector\n",
    "K = 5\n",
    "lr = 0.1\n",
    "num_epochs = 100\n",
    "\n",
    "models = [RegressionModel(Xtrain_tensor.shape[1],lr=lr) for i in range(K)]\n",
    "\n",
    "models,Xtest_tensor,ytest_tensor = create_train_test_cv_model(X,y,K,models,num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c84fed4",
   "metadata": {},
   "source": [
    "# Assessing crossvalidation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e767f820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assess model quality using r2 on test data on all K models\n",
    "# report mean and std on r2 (4 lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83047c96",
   "metadata": {},
   "source": [
    "# Visualizing coefficients of CV models\n",
    "- run the cell below to visualize your model coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c726dee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = pd.DataFrame()\n",
    "for i in range(K):\n",
    "    coef = pd.DataFrame(models[i].net.weight.data.detach().numpy(),columns=cal_df.columns[:-1])\n",
    "    coefs = pd.concat([coefs,coef],axis=0)\n",
    "coefs = coefs.reset_index(drop=True)\n",
    "\n",
    "color = {\"whiskers\": \"black\", \"medians\": \"black\", \"caps\": \"black\"}\n",
    "coefs.plot.box(vert=False, color=color)\n",
    "plt.axvline(x=0, ymin=-1, ymax=1, color=\"black\", linestyle=\"--\")\n",
    "_ = plt.title(\"Coefficients of linear regression model via 5-fold cross-validation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538ff8ef",
   "metadata": {},
   "source": [
    "# Adding weight decay and polynomial basis function expansion\n",
    "- define a new class RegressionModelL2 by copying the RegressionModel class and adding a new argument to the init function: wd (for weight decay or lambda)\n",
    "- fill in the methods below\n",
    "- you need to modify the torch.optim.SGD call to include weight_decay (see [3.7](https://d2l.ai/chapter_linear-regression/weight-decay.html#concise-implementation))\n",
    "- 5 points for the class definition below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76198d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionModelL2(nn.Module):\n",
    "    def __init__(self, num_inputs,  lr, noise=0.01,wd=0.1):\n",
    "        super(RegressionModelL2, self).__init__()\n",
    "        self.num_inputs, self.lr = num_inputs, lr\n",
    "        self.net = nn.Linear(num_inputs, 1)\n",
    "        self.weights = self.net.weight.data.normal_(0, noise)\n",
    "        self.bias = self.net.bias.data.fill_(0)\n",
    "        self.wd = wd\n",
    "        pass\n",
    "\n",
    "    def forward(self, X):\n",
    "        # your code here (1 line)\n",
    "        return self.net(X)\n",
    "    \n",
    "    def loss(self,yhat,y):\n",
    "        # your code here (1 line)\n",
    "        return self.loss(yhat,y)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # your code here (1 line)\n",
    "        return torch.optim.SGD(self.parameters(), lr=self.lr, weight_decay=self.wd)\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d17517",
   "metadata": {},
   "source": [
    "# Set up L2 regression model and train using a simple train/val/test split\n",
    "- 5 points for running the code below and assessing model quality as a function of the value of wd\n",
    "- try different values of wd and see its impact on r2 as well as on the coefficient magnitudes and signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f2b201",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cal_df.iloc[:,:-1].values # predictor matrix\n",
    "y = cal_df['price'].values    # target vector\n",
    "lr = 0.1\n",
    "num_epochs = 100\n",
    "wd = 0.01\n",
    "\n",
    "l2_model = RegressionModelL2(X.shape[1],lr=lr,wd=wd)\n",
    "l2_model,Xtest_tensor,ytest_tensor = create_train_test_model(X,y,l2_model,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16f0fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assess model quality using r2 (1 line)\n",
    "print(f\"R2 Score: {metrics.r2_score(ytest_tensor, l2_model.forward(Xtest_tensor).detach().numpy())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ae54d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot model coefficients\n",
    "coefs = pd.DataFrame(l2_model.net.weight.data.detach().numpy(),columns=cal_df.columns[:-1])\n",
    "sns.barplot(y=coefs.columns, x=coefs.iloc[0].values,orient='h')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7d78df",
   "metadata": {},
   "source": [
    "# Polynomial basis function expansion\n",
    "- use sklearn [PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) \n",
    "- 5 points for the code in this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9978feea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "X = cal_df.iloc[:,:-1]        # predictor matrix\n",
    "y = cal_df['price'].values    # target vector\n",
    "\n",
    "# make a PolynomialFeatures constructor (make include_bias = False) for polynomial of degree 2 \n",
    "# fit and transform X using that constructor, call that Xpoly\n",
    "# (2 lines)\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "Xpoly = poly.fit_transform(X)\n",
    "\n",
    "# get the feature names associated with Xpoly using the method [get_feature_names_out](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) of the constructor\n",
    "# print the shape Xpoly\n",
    "# print the feature names\n",
    "print(Xpoly.shape)\n",
    "print(poly.get_feature_names_out(X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5ebc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# understand the correlation structure of Xpoly\n",
    "Xdf = pd.DataFrame(Xpoly,columns=Xpoly_cols)\n",
    "corr_matrix = Xdf.corr().abs()\n",
    "plt.imshow(corr_matrix)\n",
    "plt.rcParams['axes.grid'] = False\n",
    "plt.colorbar()\n",
    "\n",
    "# drop the features that have correlations greater than 0.95 with other predictive features\n",
    "# get upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool_))\n",
    "\n",
    "# find features with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "\n",
    "# drop highly correlated features\n",
    "Xdf.drop(to_drop, axis=1, inplace=True)\n",
    "print(Xdf.shape)\n",
    "\n",
    "# now Xdf has just the selected features for model building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ab88b7",
   "metadata": {},
   "source": [
    "# Train the L2 model with the selected features (15 points)\n",
    "- run the cell below\n",
    "- try different weights for wd and assess the effect on model coefficients and model performance\n",
    "- what is the best setting for wd?\n",
    "- also test variation with changes in lr and num_epochs\n",
    "- does the model with expanded basis functions do better than the one without? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11268a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "num_epochs = 50\n",
    "wd = 0.1\n",
    "\n",
    "l2_model = RegressionModelL2(Xdf.shape[1],lr=lr,wd=wd)\n",
    "l2_model,Xtest_tensor,ytest_tensor = create_train_test_model(Xdf,y,l2_model,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17846c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute r2 score for l2_model after training (1 line)\n",
    "print(f\"R2 Score: {metrics.r2_score(ytest_tensor, l2_model.forward(Xtest_tensor).detach().numpy())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b19bfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot model coefficients\n",
    "coefs = pd.DataFrame(l2_model.net.weight.data.detach().numpy(),columns=Xdf.columns)\n",
    "sns.barplot(y=coefs.columns, x=coefs.iloc[0].values,orient='h')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480d58d0",
   "metadata": {},
   "source": [
    "# Congratulations! You have built unregularized and L2 regularized models in PyTorch.\n",
    "- Please leave all code and plots in this notebook and upload onto Gradescope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb86dbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
